name: Train Model

on:
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'requirements.txt'
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:

jobs:
  train:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Download Kaggle data
      env:
        KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
        KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
      run: |
        python scripts/download_data.py || echo "Data download failed, but continuing if files exist locally"
    
    - name: STEP 1 - Data Loader Agent
      id: data_loader
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        echo "::group::Data Loader Agent Output"
        python scripts/run_pipeline.py --train --no-submission || exit 1
        echo "::endgroup::"
    
    - name: STEP 2 - Feature Engineer Agent
      id: feature_engineer
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        echo "::group::Feature Engineer Agent Output"
        python -c "
from pathlib import Path
if Path('src/models/').exists():
    print('Feature engineering completed successfully')
else:
    print('Feature engineering may not have completed')
        "
        echo "::endgroup::"
    
    - name: STEP 3 - Model Trainer Agent
      id: model_trainer
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        echo "::group::Model Trainer Agent Output"
        python -c "
import joblib
from pathlib import Path
model_path = Path('src/models/baseline_model.pkl')
if model_path.exists():
    model = joblib.load(model_path)
    print(f'Model loaded successfully: {type(model).__name__}')
else:
    print('Model not found - training may have failed')
    exit(1)
        "
        echo "::endgroup::"
    
    - name: STEP 4 - Submission Agent
      id: submission
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        echo "::group::Submission Agent Output"
        python scripts/run_pipeline.py --no-train --no-detect-drift || exit 1
        echo "::endgroup::"
    
    - name: Data Drift Detection
      id: drift_detection
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        echo "::group::Data Drift Detection"
        python scripts/run_pipeline.py --detect-drift --no-train --no-submission || echo "Drift detection failed but continuing"
        echo "::endgroup::"
    
    - name: Upload model artifact
      uses: actions/upload-artifact@v4
      with:
        name: trained-model
        path: src/models/baseline_model.pkl
        if-no-files-found: warn
        retention-days: 30
    
    - name: Upload submission
      uses: actions/upload-artifact@v4
      with:
        name: submission-file
        path: submission.csv
        if-no-files-found: warn
        retention-days: 30
    
    - name: Upload reports
      uses: actions/upload-artifact@v4
      with:
        name: analysis-reports
        path: |
          reports/
          data/drift_reports/
        if-no-files-found: warn
        retention-days: 30